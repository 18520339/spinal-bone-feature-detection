{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "QZLw0nNxT-k2"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as patches\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, sampler\n",
        "from torchvision.datasets import VOCDetection\n",
        "\n",
        "import albumentations as A\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "from collections import Counter\n",
        "from tqdm.notebook import tqdm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mCH9nbfLT-k4"
      },
      "source": [
        "# Custom VOC Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "K_kc03SdT-k5"
      },
      "outputs": [],
      "source": [
        "def convert_to_yolo_format(target, img_width, img_height, class_mapping):\n",
        "    \"\"\"\n",
        "    Convert annotation data from VOC format to YOLO format.\n",
        "\n",
        "    Parameters:\n",
        "        target (dict): Annotation data from VOC dataset.\n",
        "        img_width (int): Width of the original image.\n",
        "        img_height (int): Height of the original image.\n",
        "        class_mapping (dict): Mapping from class names to integer IDs.\n",
        "\n",
        "    Returns:\n",
        "        torch.Tensor: Tensor of shape [N, 5] for N bounding boxes, each with [class_id, x_center, y_center, width, height].\n",
        "    \"\"\"\n",
        "    annotations = target['annotation']['object'] # Extract the list of annotations dictionary\n",
        "\n",
        "    # Get the real width and height of the image from the annotation.\n",
        "    real_width = int(target['annotation']['size']['width'])\n",
        "    real_height = int(target['annotation']['size']['height'])\n",
        "\n",
        "    # Ensure there is only 1 object if there're only 1 object annotations\n",
        "    if not isinstance(annotations, list): annotations = [annotations]\n",
        "    boxes = [] # Initialize an empty list to store the converted bounding boxes.\n",
        "\n",
        "    # Iterate through each annotation and convert it to YOLO format.\n",
        "    for anno in annotations:\n",
        "        xmin = int(anno['bndbox']['xmin']) / real_width\n",
        "        xmax = int(anno['bndbox']['xmax']) / real_width\n",
        "        ymin = int(anno['bndbox']['ymin']) / real_height\n",
        "        ymax = int(anno['bndbox']['ymax']) / real_height\n",
        "\n",
        "        # Calculate the center coordinates, width, and height of the bounding box.\n",
        "        x_center = (xmin + xmax) / 2\n",
        "        y_center = (ymin + ymax) / 2\n",
        "        width = xmax - xmin\n",
        "        height = ymax - ymin\n",
        "\n",
        "        # Retrieve the class name from the annotation and map it to an integer ID.\n",
        "        class_name = anno['name']\n",
        "        class_id = class_mapping[class_name] if class_name in class_mapping else 0\n",
        "        boxes.append([class_id, x_center, y_center, width, height]) # Append the YOLO formatted bounding box to the list\n",
        "    return np.array(boxes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "W0OZDcxZBqSF"
      },
      "outputs": [],
      "source": [
        "class CustomVOCDataset(VOCDetection):\n",
        "    def init_config_yolo(self, class_mapping, S=7, B=2, C=20, custom_transforms=None):\n",
        "        # Initialize YOLO-specific configuration parameters.\n",
        "        self.S = S  # Grid size S x S\n",
        "        self.B = B  # Number of bounding boxes\n",
        "        self.C = C  # Number of classes\n",
        "        self.class_mapping = class_mapping  # Mapping of class names to class indices\n",
        "        self.custom_transforms = custom_transforms\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        # Get an image and its target (annotations) from the VOC dataset.\n",
        "        image, target = super(CustomVOCDataset, self).__getitem__(index)\n",
        "        img_width, img_height = image.size\n",
        "\n",
        "        # Convert target annotations to YOLO format bounding boxes.\n",
        "        boxes = convert_to_yolo_format(target, img_width, img_height, self.class_mapping)\n",
        "        just_boxes, labels = boxes[:, 1:], boxes[:, 0]\n",
        "\n",
        "        if self.custom_transforms:\n",
        "            sample = {\n",
        "                'image': np.array(image),\n",
        "                'bboxes': just_boxes,\n",
        "                'labels': labels\n",
        "            }\n",
        "            sample = self.custom_transforms(**sample)\n",
        "            image, boxes, labels = sample['image'], sample['bboxes'], sample['labels']\n",
        "\n",
        "        # Create an empty label matrix for YOLO ground truth.\n",
        "        label_matrix = torch.zeros((self.S, self.S, self.C + 5 * self.B))\n",
        "        boxes = torch.tensor(boxes, dtype=torch.float32)\n",
        "        labels = torch.tensor(labels, dtype=torch.float32)\n",
        "        image = torch.as_tensor(image, dtype=torch.float32)\n",
        "\n",
        "        # Iterate through each bounding box in YOLO format.\n",
        "        for box, class_label in zip(boxes, labels):\n",
        "            x, y, width, height = box.tolist()\n",
        "            class_label = int(class_label)\n",
        "\n",
        "            # Calculate the grid cell (i, j) that this box belongs to.\n",
        "            i, j = int(self.S * y), int(self.S * x)\n",
        "            x_cell, y_cell = self.S * x - j, self.S * y - i\n",
        "\n",
        "            # Calculate the width and height of the box relative to the grid cell\n",
        "            width_cell, height_cell = (width * self.S, height * self.S)\n",
        "\n",
        "            # If no object has been found in this specific cell (i, j) before\n",
        "            if label_matrix[i, j, 20] == 0:\n",
        "                label_matrix[i, j, 20] = 1 # Mark that an object exists in this cell.\n",
        "\n",
        "                # Store the box coordinates as an offset from the cell boundaries\n",
        "                box_coordinates = torch.tensor([x_cell, y_cell, width_cell, height_cell])\n",
        "                label_matrix[i, j, 21:25] = box_coordinates # Set the box coordinates in the label matrix\n",
        "                label_matrix[i, j, class_label] = 1 # Set the one-hot encoding for the class label\n",
        "\n",
        "        return image, label_matrix"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9DocBZFwT-k9"
      },
      "source": [
        "# IoU, NMS, and mAP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "ksfWL2uaT-k-"
      },
      "outputs": [],
      "source": [
        "def intersection_over_union(boxes_preds, boxes_labels, box_format=\"midpoint\"):\n",
        "    \"\"\"\n",
        "    Calculate the Intersection over Union (IoU) between bounding boxes.\n",
        "\n",
        "    Parameters:\n",
        "        boxes_preds (tensor): Predicted bounding boxes (BATCH_SIZE, 4)\n",
        "        boxes_labels (tensor): Ground truth bounding boxes (BATCH_SIZE, 4)\n",
        "        box_format (str): box format, can be \"midpoint\" or \"corners\".\n",
        "\n",
        "    Returns:\n",
        "        tensor: Intersection over Union scores for each example.\n",
        "    \"\"\"\n",
        "    if box_format == \"midpoint\":\n",
        "        # Calculate coordinates of top-left (x1, y1) and bottom-right (x2, y2) points for predicted boxes\n",
        "        box1_x1 = boxes_preds[..., 0:1] - boxes_preds[..., 2:3] / 2\n",
        "        box1_y1 = boxes_preds[..., 1:2] - boxes_preds[..., 3:4] / 2\n",
        "        box1_x2 = boxes_preds[..., 0:1] + boxes_preds[..., 2:3] / 2\n",
        "        box1_y2 = boxes_preds[..., 1:2] + boxes_preds[..., 3:4] / 2\n",
        "\n",
        "        # Calculate coordinates of top-left (x1, y1) and bottom-right (x2, y2) points for ground truth boxes\n",
        "        box2_x1 = boxes_labels[..., 0:1] - boxes_labels[..., 2:3] / 2\n",
        "        box2_y1 = boxes_labels[..., 1:2] - boxes_labels[..., 3:4] / 2\n",
        "        box2_x2 = boxes_labels[..., 0:1] + boxes_labels[..., 2:3] / 2\n",
        "        box2_y2 = boxes_labels[..., 1:2] + boxes_labels[..., 3:4] / 2\n",
        "\n",
        "    if box_format == \"corners\":\n",
        "        # Extract coordinates for predicted boxes\n",
        "        box1_x1 = boxes_preds[..., 0:1]\n",
        "        box1_y1 = boxes_preds[..., 1:2]\n",
        "        box1_x2 = boxes_preds[..., 2:3]\n",
        "        box1_y2 = boxes_preds[..., 3:4]\n",
        "\n",
        "        # Extract coordinates for ground truth boxes\n",
        "        box2_x1 = boxes_labels[..., 0:1]\n",
        "        box2_y1 = boxes_labels[..., 1:2]\n",
        "        box2_x2 = boxes_labels[..., 2:3]\n",
        "        box2_y2 = boxes_labels[..., 3:4]\n",
        "\n",
        "    # Calculate the intersection rectangle\n",
        "    x1 = torch.max(box1_x1, box2_x1)\n",
        "    y1 = torch.max(box1_y1, box2_y1)\n",
        "    x2 = torch.min(box1_x2, box2_x2)\n",
        "    y2 = torch.min(box1_y2, box2_y2)\n",
        "\n",
        "    # Compute the area of the intersection rectangle, clamp(0) to handle cases where they do not overlap\n",
        "    intersection = (x2 - x1).clamp(0) * (y2 - y1).clamp(0)\n",
        "\n",
        "    # Calculate the areas of the predicted and ground truth boxes\n",
        "    box1_area = abs((box1_x2 - box1_x1) * (box1_y2 - box1_y1))\n",
        "    box2_area = abs((box2_x2 - box2_x1) * (box2_y2 - box2_y1))\n",
        "\n",
        "    # Calculate the IoU, adding a small epsilon to avoid division by 0\n",
        "    return intersection / (box1_area + box2_area - intersection + 1e-6)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "qDy6RkY8T-k_"
      },
      "outputs": [],
      "source": [
        "def non_maximum_suppression(boxes, iou_threshold, threshold, box_format=\"corners\"):\n",
        "    \"\"\"\n",
        "    Perform Non-Maximum Suppression on a list of bounding boxes.\n",
        "\n",
        "    Parameters:\n",
        "        boxes (list): List of bounding boxes, each represented as [class_pred, prob_score, x1, y1, x2, y2].\n",
        "        iou_threshold (float): IoU threshold to determine correct predicted bounding boxes.\n",
        "        threshold (float): Threshold to discard predicted bounding boxes (independent of IoU).\n",
        "        box_format (str): \"midpoint\" or \"corners\" to specify the format of bounding boxes.\n",
        "\n",
        "    Returns:\n",
        "        list: List of bounding boxes after performing NMS with a specific IoU threshold.\n",
        "    \"\"\"\n",
        "    assert type(boxes) == list # Check the data type of the input\n",
        "    boxes = [box for box in boxes if box[1] > threshold] # Filter predicted boxes based on probability threshold\n",
        "    boxes = sorted(boxes, key=lambda x: x[1], reverse=True) # Sort boxes by probability in descending order\n",
        "    boxes_after_nms = [] # List to store bounding boxes after NMS\n",
        "\n",
        "    while boxes: # Continue looping until the list of bounding boxes is empty\n",
        "        chosen_box = boxes.pop(0) # Get the bounding box with the highest probability\n",
        "\n",
        "        # Remove bounding boxes with IoU greater than the specified threshold with the chosen box\n",
        "        boxes = [box for box in boxes if box[0] != chosen_box[0] or intersection_over_union(\n",
        "            torch.tensor(chosen_box[2:]), torch.tensor(box[2:]), box_format=box_format\n",
        "        ) < iou_threshold]\n",
        "        boxes_after_nms.append(chosen_box) # Add the chosen bounding box to the list after NMS\n",
        "    return boxes_after_nms # Return the list of bounding boxes after NMS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "sIEwGzzjT-k_"
      },
      "outputs": [],
      "source": [
        "def mean_average_precision(pred_boxes, true_boxes, iou_threshold=0.5, box_format=\"midpoint\", num_classes=20):\n",
        "    \"\"\"\n",
        "    Calculate the mean average precision (mAP).\n",
        "\n",
        "    Parameters:\n",
        "        pred_boxes (list): A list containing predicted bounding boxes with each box defined as [train_idx, class_pred, prob_score, x1, y1, x2, y2].\n",
        "        true_boxes (list): Similar to pred_boxes but containing information about true boxes.\n",
        "        iou_threshold (float): IoU threshold, where the predicted boxes are considered correct.\n",
        "        box_format (str): \"midpoint\" or \"corners\" used to specify the format of boxes.\n",
        "        num_classes (int): Number of classes.\n",
        "\n",
        "    Returns:\n",
        "        float: The mAP value across all classes with a specific IoU threshold.\n",
        "    \"\"\"\n",
        "    average_precisions = [] # List to store mAP for each class\n",
        "    epsilon = 1e-6\n",
        "\n",
        "    for c in range(num_classes):\n",
        "        detections, ground_truths = [], []\n",
        "\n",
        "        # Iterate through predicted boxes and true boxes, and only add those belonging to the current class 'c'\n",
        "        for detection in pred_boxes:\n",
        "            if detection[1] == c: detections.append(detection)\n",
        "\n",
        "        for true_box in true_boxes:\n",
        "            if true_box[1] == c: ground_truths.append(true_box)\n",
        "\n",
        "        # Find the number of boxes for ech training example\n",
        "        # The Counter here counts the number of target boxes we have for each training example\n",
        "        # So if image 0 has 3, and image 1 has 5, we'll have a dict like {0: 3, 1: 5}\n",
        "        amount_boxes = Counter([gt[0] for gt in ground_truths])\n",
        "\n",
        "        # Loop through each key, val in the above dict and convert it to the following (for the same example):\n",
        "        # {0: [0, 0, 0], 1: [0, 0, 0]} where the number of zeros is equal to the number of boxes in the image\n",
        "        for key, val in amount_boxes.items():\n",
        "            amount_boxes[key] = torch.zeros(val)\n",
        "\n",
        "        # Sort by box probability, index 2 is the probability\n",
        "        detections.sort(key=lambda x: x[2], reverse=True)\n",
        "        TP = torch.zeros((len(detections)))\n",
        "        FP = torch.zeros((len(detections)))\n",
        "        total_true_bboxes = len(ground_truths)\n",
        "\n",
        "        if total_true_bboxes == 0: continue # If there are no true boxes for this class, it can be safely skipped\n",
        "        for detection_idx, detection in enumerate(detections):\n",
        "            # Only consider ground truth boxes with the same training index as the prediction\n",
        "            ground_truth_img = [bbox for bbox in ground_truths if bbox[0] == detection[0]]\n",
        "            best_iou = 0\n",
        "\n",
        "            for idx, gt in enumerate(ground_truth_img):\n",
        "                iou = intersection_over_union(torch.tensor(detection[3:]), torch.tensor(gt[3:]), box_format=box_format)\n",
        "                if iou > best_iou:\n",
        "                    best_iou = iou\n",
        "                    best_gt_idx = idx\n",
        "\n",
        "            if best_iou > iou_threshold:\n",
        "                if amount_boxes[detection[0]][best_gt_idx] == 0: # Only detect ground truth boxes once\n",
        "                    TP[detection_idx] = 1 # True positive and mark this box as seen\n",
        "                    amount_boxes[detection[0]][best_gt_idx] = 1\n",
        "                else: FP[detection_idx] = 1\n",
        "            else: FP[detection_idx] = 1 # If IoU is lower, the detection result is a false positive\n",
        "\n",
        "        TP_cumsum = torch.cumsum(TP, dim=0)\n",
        "        FP_cumsum = torch.cumsum(FP, dim=0)\n",
        "        recalls = TP_cumsum / (total_true_bboxes + epsilon)\n",
        "\n",
        "        precisions = torch.divide(TP_cumsum, (TP_cumsum + FP_cumsum + epsilon))\n",
        "        precisions = torch.cat((torch.tensor([1]), precisions))\n",
        "        recalls = torch.cat((torch.tensor([0]), recalls))\n",
        "        average_precisions.append(torch.trapz(precisions, recalls)) # Use torch.trapz for numerical integration\n",
        "    return sum(average_precisions) / len(average_precisions)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Bounding Boxes Utilities"
      ],
      "metadata": {
        "id": "nLpN3XkCbhYa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def cellboxes_to_boxes(predictions):\n",
        "    \"\"\"\n",
        "    Convert model output from cell grid format to bounding boxes.\n",
        "\n",
        "    Parameters:\n",
        "        predictions (tensor): Model predictions, shape (batch_size, S*S*(C+B*5)) or already reshaped to (batch_size, S, S, C+B*5)\n",
        "\n",
        "    Returns:\n",
        "        list: List of bounding boxes for each image in the batch, where each box is [class_pred, prob_score, x, y, width, height]\n",
        "    \"\"\"\n",
        "    batch_size = predictions.shape[0]\n",
        "    S = 7  # Grid size\n",
        "    B = 2  # Number of boxes per cell\n",
        "    C = 20  # Number of classes\n",
        "    predictions = predictions.reshape(batch_size, S, S, C + B * 5) # Reshape predictions if needed\n",
        "    all_bboxes = [] # Lists to store all bounding boxes for each image\n",
        "\n",
        "\n",
        "    for i in range(batch_size): # For each image in the batch\n",
        "        bboxes = []\n",
        "        for row in range(S): # Iterate through each cell in the grid\n",
        "            for col in range(S):\n",
        "                class_probs = predictions[i, row, col, :C] # Get class probabilities\n",
        "\n",
        "                box1_confidence = predictions[i, row, col, C] # Process first box\n",
        "                box1_x = (predictions[i, row, col, C+1] + col) / S\n",
        "                box1_y = (predictions[i, row, col, C+2] + row) / S\n",
        "                box1_w = predictions[i, row, col, C+3]\n",
        "                box1_h = predictions[i, row, col, C+4]\n",
        "\n",
        "                box2_confidence = predictions[i, row, col, C+5] # Process second box\n",
        "                box2_x = (predictions[i, row, col, C+6] + col) / S\n",
        "                box2_y = (predictions[i, row, col, C+7] + row) / S\n",
        "                box2_w = predictions[i, row, col, C+8]\n",
        "                box2_h = predictions[i, row, col, C+9]\n",
        "\n",
        "                for c in range(C): # For each class\n",
        "                    prob_box1 = class_probs[c] * box1_confidence # Probability for class c with box 1\n",
        "                    if prob_box1 > 0: # Add box 1 if probability is non-zero\n",
        "                        bboxes.append([c, prob_box1.item(), box1_x.item(), box1_y.item(), box1_w.item(), box1_h.item()])\n",
        "\n",
        "                    prob_box2 = class_probs[c] * box2_confidence # Probability for class c with box 2\n",
        "                    if prob_box2 > 0: # Add box 2 if probability is non-zero\n",
        "                        bboxes.append([c, prob_box2.item(), box2_x.item(), box2_y.item(), box2_w.item(), box2_h.item()])\n",
        "        all_bboxes.append(bboxes)\n",
        "    return all_bboxes"
      ],
      "metadata": {
        "id": "0v60VVE-bnHx"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_bboxes_training(predictions, targets, iou_threshold, threshold):\n",
        "    \"\"\"\n",
        "    Get bounding boxes from predictions and targets for mAP calculation during training.\n",
        "\n",
        "    Parameters:\n",
        "        predictions (tensor): Model predictions\n",
        "        targets (tensor): Ground truth targets\n",
        "        iou_threshold (float): IoU threshold for NMS\n",
        "        threshold (float): Confidence threshold for predictions\n",
        "\n",
        "    Returns:\n",
        "        tuple: (pred_boxes, target_boxes) where each is a list of bounding boxes in the format\n",
        "               required by mean_average_precision function\n",
        "    \"\"\"\n",
        "    # Convert predictions and targets to bounding boxes\n",
        "    pred_boxes_raw = cellboxes_to_boxes(predictions)\n",
        "    target_boxes_raw = cellboxes_to_boxes(targets)\n",
        "    all_pred_boxes, all_target_boxes = [], [] # Initialize lists to store processed boxes\n",
        "    batch_size = predictions.shape[0]\n",
        "\n",
        "    for idx in range(batch_size): # Process each image in the batch\n",
        "        nms_boxes = non_maximum_suppression(pred_boxes_raw[idx], iou_threshold=iou_threshold, threshold=threshold, box_format=\"midpoint\")\n",
        "        for nms_box in nms_boxes: # Add batch index to each box for mAP calculation\n",
        "            all_pred_boxes.append([idx] + nms_box)\n",
        "\n",
        "        for target_box in target_boxes_raw[idx]: # Process target boxes (no need for NMS, but add batch index)\n",
        "            if target_box[1] > threshold: # Only include boxes with non-zero confidence\n",
        "                all_target_boxes.append([idx] + target_box)\n",
        "\n",
        "    return all_pred_boxes, all_target_boxes"
      ],
      "metadata": {
        "id": "SEgbh2-MbrUo"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rQA2uBkRT-lA"
      },
      "source": [
        "# YOLOv1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "b9BOu7mdT-lA"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Information about the architectural configuration:\n",
        "A Tuple is structured as (kernel_size, number of filters, stride, padding).\n",
        "\"M\" simply represents max-pooling with a 2x2 pool filter size and 2x2 kernel.\n",
        "The list is structured according to the data blocks, and ends with an integer representing the number of repetitions.\n",
        "\"\"\"\n",
        "\n",
        "# Describing convolutional and max-pooling layers, as well as the number of repetitions for convolutional blocks.\n",
        "architecture_config = [\n",
        "    (7, 64, 2, 3),   # Convolutional block 1\n",
        "    \"M\", # Max-pooling layer 1\n",
        "    (3, 192, 1, 1),  # Convolutional block 2\n",
        "    \"M\", # Max-pooling layer 2\n",
        "    (1, 128, 1, 0),  # Convolutional block 3\n",
        "    (3, 256, 1, 1),  # Convolutional block 4\n",
        "    (1, 256, 1, 0),  # Convolutional block 5\n",
        "    (3, 512, 1, 1),  # Convolutional block 6\n",
        "    \"M\", # Max-pooling layer 3\n",
        "   [(1, 256, 1, 0), (3, 512, 1, 1), 4],  # Convolutional block 7 (repeated 4 times)\n",
        "    (1, 512, 1, 0),  # Convolutional block 8\n",
        "    (3, 1024, 1, 1), # Convolutional block 9\n",
        "    \"M\", # Max-pooling layer 4\n",
        "   [(1, 512, 1, 0), (3, 1024, 1, 1), 2], # Convolutional block 10 (repeated 2 times)\n",
        "    (3, 1024, 1, 1), # Convolutional block 11\n",
        "    (3, 1024, 2, 1), # Convolutional block 12\n",
        "    (3, 1024, 1, 1), # Convolutional block 13\n",
        "    (3, 1024, 1, 1), # Convolutional block 14\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "MPTCAaeyT-lB"
      },
      "outputs": [],
      "source": [
        "class CNNBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, **kwargs):\n",
        "        super(CNNBlock, self).__init__()\n",
        "        self.conv = nn.Conv2d(in_channels, out_channels, bias=False, **kwargs)\n",
        "        self.batchnorm = nn.BatchNorm2d(out_channels)\n",
        "        self.leakyrelu = nn.LeakyReLU(0.1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.leakyrelu(self.batchnorm(self.conv(x)))\n",
        "\n",
        "\n",
        "class YOLOv1(nn.Module): # The YOLOv1 model is defined with conv layers and fully connected layers\n",
        "    def __init__(self, in_channels=3, **kwargs):\n",
        "        super(YOLOv1, self).__init__()\n",
        "        self.architecture = architecture_config\n",
        "        self.in_channels = in_channels\n",
        "        self.darknet = self._create_conv_layers(self.architecture)\n",
        "        self.fcs = self._create_fcs(**kwargs)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.darknet(x)\n",
        "        return self.fcs(torch.flatten(x, start_dim=1))\n",
        "\n",
        "\n",
        "    # Function to create conv layers based on the predefined architecture configuration\n",
        "    def _create_conv_layers(self, architecture):\n",
        "        layers = []\n",
        "        in_channels = self.in_channels\n",
        "\n",
        "        for x in architecture:\n",
        "            if type(x) == tuple:\n",
        "                layers += [CNNBlock(in_channels, x[1], kernel_size=x[0], stride=x[2], padding=x[3])]\n",
        "                in_channels = x[1]\n",
        "            elif type(x) == str:\n",
        "                layers += [nn.MaxPool2d(kernel_size=(2, 2), stride=(2, 2))]\n",
        "            elif type(x) == list:\n",
        "                conv1, conv2 = x[0], x[1]\n",
        "                num_repeats = x[2]\n",
        "                for _ in range(num_repeats):\n",
        "                    layers += [CNNBlock(in_channels, conv1[1], kernel_size=conv1[0], stride=conv1[2], padding=conv1[3])]\n",
        "                    layers += [CNNBlock(conv1[1], conv2[1], kernel_size=conv2[0], stride=conv2[2], padding=conv2[3])]\n",
        "                    in_channels = conv2[1]\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    # Function to create fully connected layers based on input parameters such as grid size, number of boxes, and number of classes\n",
        "    def _create_fcs(self, split_size, num_boxes, num_classes):\n",
        "        S, B, C = split_size, num_boxes, num_classes\n",
        "        return nn.Sequential(\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(1024 * S * S, 4096),\n",
        "            nn.LeakyReLU(0.1),\n",
        "            nn.Linear(4096, S * S * (C + B * 5)),\n",
        "        )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "gw2OGy8ET-lB"
      },
      "outputs": [],
      "source": [
        "class YoloLoss(nn.Module): # Calculate the loss for the YOLO (v1) model\n",
        "    def __init__(self, S=7, B=2, C=20):\n",
        "        super(YoloLoss, self).__init__()\n",
        "        self.mse = nn.MSELoss(reduction=\"sum\")\n",
        "        \"\"\"\n",
        "        S is the grid size of the image (7),\n",
        "        B is the number of bounding boxes (2),\n",
        "        C is the number of classes (in PASCAL VOC dataset, it's 20).\n",
        "        \"\"\"\n",
        "        self.S = S\n",
        "        self.B = B\n",
        "        self.C = C\n",
        "\n",
        "        # These are YOLO-specific constants, representing the weight\n",
        "        # for no object (lambda_noobj) and box coordinates loss (lambda_coord).\n",
        "        self.lambda_noobj = 0.5\n",
        "        self.lambda_coord = 5\n",
        "\n",
        "\n",
        "    def forward(self, predictions, target):\n",
        "        # Reshape the predictions to the shape (BATCH_SIZE, S * S * (C + B * 5))\n",
        "        predictions = predictions.reshape(-1, self.S, self.S, self.C + self.B * 5)\n",
        "\n",
        "        # Calculate IoU for the 2 predicted bounding boxes with the target bounding box.\n",
        "        iou_b1 = intersection_over_union(predictions[..., 21:25], target[..., 21:25])\n",
        "        iou_b2 = intersection_over_union(predictions[..., 26:30], target[..., 21:25])\n",
        "        ious = torch.cat([iou_b1.unsqueeze(0), iou_b2.unsqueeze(0)], dim=0)\n",
        "\n",
        "        # Get the box with the highest IoU among the 2 predictions.\n",
        "        # Note that bestbox will have an index of 0 or 1, indicating which box is better.\n",
        "        iou_maxes, bestbox = torch.max(ious, dim=0)\n",
        "        exists_box = target[..., 20].unsqueeze(3)  # This represents Iobj_i in the paper\n",
        "\n",
        "        # FOR BOX COORDINATES\n",
        "        # Set the boxes with no objects to 0. Choose 1 of the 2 predictions based on the bestbox index calculated earlier.\n",
        "        box_predictions = exists_box * (bestbox * predictions[..., 26:30] + (1 - bestbox) * predictions[..., 21:25])\n",
        "        box_targets = exists_box * target[..., 21:25]\n",
        "\n",
        "        # Take the square root of width and height to ensure positive values.\n",
        "        box_predictions[..., 2:4] = torch.sign(box_predictions[..., 2:4]) * torch.sqrt(torch.abs(box_predictions[..., 2:4] + 1e-6))\n",
        "        box_targets[..., 2:4] = torch.sqrt(box_targets[..., 2:4])\n",
        "        box_loss = self.mse(torch.flatten(box_predictions, end_dim=-2), torch.flatten(box_targets, end_dim=-2))\n",
        "\n",
        "        # FOR OBJECT LOSS\n",
        "        # pred_box represents the confidence score of the box with the highest IoU.\n",
        "        pred_box = bestbox * predictions[..., 25:26] + (1 - bestbox) * predictions[..., 20:21]\n",
        "        object_loss = self.mse(torch.flatten(exists_box * pred_box), torch.flatten(exists_box * target[..., 20:21]))\n",
        "\n",
        "        # FOR NO OBJECT LOSS\n",
        "        no_object_loss = self.mse(\n",
        "            torch.flatten((1 - exists_box) * predictions[..., 20:21], start_dim=1),\n",
        "            torch.flatten((1 - exists_box) * target[..., 20:21], start_dim=1),\n",
        "        )\n",
        "        no_object_loss += self.mse(\n",
        "            torch.flatten((1 - exists_box) * predictions[..., 25:26], start_dim=1),\n",
        "            torch.flatten((1 - exists_box) * target[..., 20:21], start_dim=1),\n",
        "        )\n",
        "\n",
        "        # FOR CLASS LOSS\n",
        "        class_loss = self.mse(\n",
        "            torch.flatten(exists_box * predictions[..., :20], end_dim=-2),\n",
        "            torch.flatten(exists_box * target[..., :20], end_dim=-2),\n",
        "        )\n",
        "\n",
        "        # Calculate the final loss by combining the above components.\n",
        "        loss = (\n",
        "            self.lambda_coord * box_loss  # First term\n",
        "            + object_loss  # Second term\n",
        "            + self.lambda_noobj * no_object_loss  # Third term\n",
        "            + class_loss  # Fourth term\n",
        "        )\n",
        "        return loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "htc0kaSYT-lC"
      },
      "source": [
        "# Training Constants"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "Z5j8QvsuT-lC"
      },
      "outputs": [],
      "source": [
        "torch.manual_seed(123)\n",
        "LEARNING_RATE = 2e-5\n",
        "\n",
        "# Specify whether to use \"cuda\" (GPU) or \"cpu\" for training.\n",
        "# \"cuda\" in the research paper, but using a smaller batch size due to GPU memory limits.\n",
        "DEVICE = \"cuda\"\n",
        "BATCH_SIZE = 32\n",
        "EPOCHS = 10\n",
        "NUM_WORKERS = 2 # Number of worker processes for the data loader.\n",
        "PIN_MEMORY = True # Pin memory will speed up the process of transferring data to the GPU faster.\n",
        "LOAD_MODEL = False # If False, the training process will not load a pre-trained model.\n",
        "LOAD_MODEL_FILE = \"yolov1.pth.tar\" # Specify the file name for the pre-trained model if LOAD_MODEL is True.\n",
        "WIDTH = 448\n",
        "HEIGHT = 448"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "Dt1W46VvT-lC"
      },
      "outputs": [],
      "source": [
        "class_mapping = {\n",
        "    \"aeroplane\": 0,\n",
        "    \"bicycle\": 1,\n",
        "    \"bird\": 2,\n",
        "    \"boat\": 3,\n",
        "    \"bottle\": 4,\n",
        "    \"bus\": 5,\n",
        "    \"car\": 6,\n",
        "    \"cat\": 7,\n",
        "    \"chair\": 8,\n",
        "    \"cow\": 9,\n",
        "    \"diningtable\": 10,\n",
        "    \"dog\": 11,\n",
        "    \"horse\": 12,\n",
        "    \"motorbike\": 13,\n",
        "    \"person\": 14,\n",
        "    \"pottedplant\": 15,\n",
        "    \"sheep\": 16,\n",
        "    \"sofa\": 17,\n",
        "    \"train\": 18,\n",
        "    \"tvmonitor\": 19\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DSfJMjqET-lC"
      },
      "source": [
        "# Prepare the Data for Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "Rc7pWJcMT-lC"
      },
      "outputs": [],
      "source": [
        "def get_train_transforms():\n",
        "    return A.Compose([\n",
        "        A.OneOf([\n",
        "            A.HueSaturationValue(hue_shift_limit=0.2, sat_shift_limit=0.2, val_shift_limit=0.2, p=0.9),\n",
        "            A.RandomBrightnessContrast(brightness_limit=0.2, contrast_limit=0.2, p=0.9)\n",
        "        ], p=0.9),\n",
        "        A.ToGray(p=0.01),\n",
        "        A.HorizontalFlip(p=0.2),\n",
        "        A.VerticalFlip(p=0.2),\n",
        "        A.Resize(height=HEIGHT, width=WIDTH, p=1),\n",
        "        # A.Cutout(num_holes=8, max_h_size=64, max_w_size=64, fill_value=0, p=0.5),\n",
        "        ToTensorV2(p=1.0),\n",
        "    ], p=1.0, bbox_params=A.BboxParams(format=\"yolo\", min_area=0, min_visibility=0, label_fields=[\"labels\"]))\n",
        "\n",
        "\n",
        "def get_valid_transforms():\n",
        "    return A.Compose([\n",
        "        A.Resize(height=HEIGHT, width=WIDTH, p=1.0),\n",
        "        ToTensorV2(p=1.0),\n",
        "    ], p=1.0, bbox_params=A.BboxParams(format=\"yolo\", min_area=0, min_visibility=0, label_fields=[\"labels\"]))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = CustomVOCDataset(root=\"./data\", year=\"2012\", image_set=\"train\", download=True)\n",
        "val_dataset = CustomVOCDataset(root=\"./data\", year=\"2012\", image_set=\"val\", download=True)\n",
        "train_dataset.init_config_yolo(class_mapping=class_mapping, custom_transforms=get_train_transforms())\n",
        "val_dataset.init_config_yolo(class_mapping=class_mapping, custom_transforms=get_valid_transforms())"
      ],
      "metadata": {
        "id": "kYmJmexjo0rk"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "QRXiDQwZT-lC"
      },
      "outputs": [],
      "source": [
        "# Split dataset into train, validation, and test sets using indices\n",
        "dataset_size = len(val_dataset)\n",
        "val_size = int(0.15 * dataset_size)\n",
        "val_sampler = sampler.SubsetRandomSampler(range(val_size))\n",
        "test_sampler = sampler.SubsetRandomSampler(range(val_size, dataset_size))\n",
        "\n",
        "# Create data loaders for training, validation, and test sets\n",
        "train_loader = DataLoader(\n",
        "    dataset=train_dataset, batch_size=BATCH_SIZE,\n",
        "    num_workers=NUM_WORKERS, pin_memory=PIN_MEMORY, drop_last=True\n",
        ")\n",
        "val_loader = DataLoader(\n",
        "    dataset=val_dataset, batch_size=BATCH_SIZE,\n",
        "    num_workers=NUM_WORKERS, pin_memory=PIN_MEMORY,\n",
        "    sampler=val_sampler, drop_last=False\n",
        ")\n",
        "test_loader = DataLoader(\n",
        "    dataset=val_dataset, batch_size=BATCH_SIZE,\n",
        "    num_workers=NUM_WORKERS, pin_memory=PIN_MEMORY,\n",
        "    sampler=test_sampler, drop_last=False\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KU-BinMyT-lD"
      },
      "source": [
        "# Training Utilities"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "DT3pZSJkT-lD"
      },
      "outputs": [],
      "source": [
        "def train_fn(train_loader, model, optimizer, loss_fn, epoch):\n",
        "    mean_loss,mean_mAP = [], []\n",
        "    total_batches = len(train_loader)\n",
        "    loop = tqdm(enumerate(train_loader), total=total_batches)\n",
        "\n",
        "    for batch_idx, (x, y) in loop:\n",
        "        x, y = x.to(DEVICE), y.to(DEVICE)\n",
        "        out = model(x)\n",
        "        loss = loss_fn(out, y)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        pred_boxes, true_boxes = get_bboxes_training(out, y, iou_threshold=0.5, threshold=0.4)\n",
        "        mAP = mean_average_precision(pred_boxes, true_boxes, iou_threshold=0.5, box_format=\"midpoint\")\n",
        "        mean_loss.append(loss.item())\n",
        "        mean_mAP.append(mAP.item())\n",
        "\n",
        "        # Update the progress bar\n",
        "        loop.set_description(f\"[EPOCH {epoch + 1}/{EPOCHS}] {batch_idx + 1}/{total_batches}\")\n",
        "        loop.set_postfix(loss=loss.item(), mAP=mAP.item())\n",
        "\n",
        "    avg_loss = sum(mean_loss) / len(mean_loss)\n",
        "    avg_mAP = sum(mean_mAP) / len(mean_mAP)\n",
        "    return avg_loss, avg_mAP\n",
        "\n",
        "\n",
        "def val_fn(test_loader, model, loss_fn):\n",
        "    model.eval()\n",
        "    mean_loss, mean_mAP = [], []\n",
        "\n",
        "    for x, y in test_loader:\n",
        "        x, y = x.to(DEVICE), y.to(DEVICE)\n",
        "        out = model(x)\n",
        "        loss = loss_fn(out, y)\n",
        "\n",
        "        pred_boxes, true_boxes = get_bboxes_training(out, y, iou_threshold=0.5, threshold=0.4)\n",
        "        mAP = mean_average_precision(pred_boxes, true_boxes, iou_threshold=0.5, box_format=\"midpoint\")\n",
        "        mean_loss.append(loss.item())\n",
        "        mean_mAP.append(mAP.item())\n",
        "\n",
        "    avg_loss = sum(mean_loss) / len(mean_loss)\n",
        "    avg_mAP = sum(mean_mAP) / len(mean_mAP)\n",
        "    model.train()\n",
        "    return avg_loss, avg_mAP"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wWUnJsrpT-lD"
      },
      "source": [
        "# Experiments"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "0kbMR0QTT-lE"
      },
      "outputs": [],
      "source": [
        "model = YOLOv1(split_size=7, num_boxes=2, num_classes=20).to(DEVICE)\n",
        "optimizer = optim.AdamW(model.parameters(), lr=LEARNING_RATE)\n",
        "loss_fn = YoloLoss()\n",
        "\n",
        "if LOAD_MODEL: # Load checkpoint if necessary\n",
        "    checkpoint = torch.load(LOAD_MODEL_FILE)\n",
        "    model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
        "    optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lAS6fnssT-lE",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "0fdca13d6e364fedbe382ba105c06e3b",
            "47e39aa4a6854241b634bc1a170766c4",
            "00c25d8bfe694d82b0478286ac73a746",
            "aa4f666aab9a4700b4927fa3b6220af5",
            "f2c944dea18a413fb6e07c341e137ffe",
            "aae4c681ca224d41aa9d4001b0da8a54",
            "a541b53a0bbd44bd83e88a63c8ff92b9",
            "04e531afc70045f8a7c5612c8e1b2580",
            "670cc6f44204479db40653b83b14ab73",
            "fcb81569359a4b1baccc7e0c785ddb29",
            "3dfec5bdf9ec4197971509afc2345974"
          ]
        },
        "outputId": "797d8b63-0657-47f6-aecf-36788a4dd6e7"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/178 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0fdca13d6e364fedbe382ba105c06e3b"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "%%time\n",
        "best_mAP_train, best_mAP_val = 0, 0\n",
        "for epoch in range(EPOCHS):\n",
        "    train_loss, train_mAP = train_fn(train_loader, model, optimizer, loss_fn, epoch)\n",
        "    val_loss, val_mAP = val_fn(val_loader, model, loss_fn)\n",
        "    print(f\"=> Loss: {train_loss:.4f} - mAP: {train_mAP:.4f} - Val Loss: {val_loss:.4f} - Val mAP: {val_mAP:.4f}\")\n",
        "\n",
        "    if train_mAP > best_mAP_train: best_mAP_train = train_mAP # Update best mAP values\n",
        "    if val_mAP > best_mAP_val: # Save checkpoint when validation mAP improves\n",
        "        best_mAP_val = val_mAP\n",
        "        checkpoint = {\n",
        "            \"state_dict\": model.state_dict(),\n",
        "            \"optimizer\": optimizer.state_dict(),\n",
        "        }\n",
        "        torch.save(checkpoint, LOAD_MODEL_FILE)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gc\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "qh_8ctTZZQQS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qyl-g_agT-lE"
      },
      "source": [
        "# Inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lOwfjcbOT-lE"
      },
      "outputs": [],
      "source": [
        "# Draw both ground truth and predicted boxes on the image with labels\n",
        "def plot_image_with_labels(image, ground_truth_boxes, predicted_boxes, class_mapping):\n",
        "    inverse_class_mapping = {v: k for k, v in class_mapping.items()} # Invert class mapping for easy access of class names based on indices\n",
        "    im = np.array(image) # Convert the image to a numpy array and get its dimensions\n",
        "    height, width, _ = im.shape\n",
        "    fig, ax = plt.subplots(1) # Create a figure and axis for plotting\n",
        "    ax.imshow(im)\n",
        "\n",
        "    for box in ground_truth_boxes: # Plot each ground truth box in green\n",
        "        label_index, box = box[0], box[2:] # Extract label index and box coordinates\n",
        "\n",
        "        # Calculate upper left coordinates\n",
        "        upper_left_x = box[0] - box[2] / 2\n",
        "        upper_left_y = box[1] - box[3] / 2\n",
        "\n",
        "        # Create a rectangle patch\n",
        "        rect = patches.Rectangle(\n",
        "            (upper_left_x * width, upper_left_y * height),\n",
        "            box[2] * width, box[3] * height,\n",
        "            linewidth=1, edgecolor=\"green\", facecolor=\"none\",\n",
        "        )\n",
        "        ax.add_patch(rect) # Add the rectangle to the plot\n",
        "\n",
        "        # Retrieve the class name and add it as text to the plot\n",
        "        class_name = inverse_class_mapping.get(label_index, \"Unknown\")\n",
        "        ax.text(upper_left_x * width, upper_left_y * height, class_name, color=\"white\", fontsize=12, bbox=dict(facecolor=\"green\", alpha=0.2))\n",
        "\n",
        "    for box in predicted_boxes: # Plot each predicted box in red (Similar processing as for ground truth boxes)\n",
        "        label_index, box = box[0], box[2:]\n",
        "        upper_left_x = box[0] - box[2] / 2\n",
        "        upper_left_y = box[1] - box[3] / 2\n",
        "        rect = patches.Rectangle(\n",
        "            (upper_left_x * width, upper_left_y * height),\n",
        "            box[2] * width, box[3] * height,\n",
        "            linewidth=1, edgecolor=\"red\", facecolor=\"none\",\n",
        "        )\n",
        "        ax.add_patch(rect)\n",
        "        class_name = inverse_class_mapping.get(label_index, \"Unknown\")\n",
        "        ax.text(upper_left_x * width, upper_left_y * height, class_name, color=\"white\", fontsize=12, bbox=dict(facecolor=\"red\", alpha=0.2))\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1nP4hGmfT-lE"
      },
      "outputs": [],
      "source": [
        "model = YOLOv1(split_size=7, num_boxes=2, num_classes=20).to(DEVICE)\n",
        "if LOAD_MODEL: model.load_state_dict(torch.load(LOAD_MODEL_FILE)[\"state_dict\"])\n",
        "model.eval()\n",
        "\n",
        "# Iterate over the test dataset and process each batch\n",
        "for x, y in test_loader:\n",
        "    x = x.to(DEVICE)\n",
        "    out = model(x)\n",
        "\n",
        "    # Convert model output to bounding boxes and apply non-max suppression\n",
        "    pred_boxes = cellboxes_to_boxes(out)\n",
        "    gt_boxes = cellboxes_to_boxes(y)\n",
        "\n",
        "    # Plot the first 8 images with their ground truth and predicted bounding boxes\n",
        "    for idx in range(8):\n",
        "        pred_box = non_maximum_suppression(pred_boxes[idx], iou_threshold=0.5, threshold=0.4, box_format=\"midpoint\")\n",
        "        gt_box = non_maximum_suppression(gt_boxes[idx], iou_threshold=0.5, threshold=0.4, box_format=\"midpoint\")\n",
        "        image = x[idx].permute(1, 2, 0).to(\"cpu\") / 255.0\n",
        "        plot_image_with_labels(image, gt_box, pred_box, class_mapping)\n",
        "    break # Stop after processing the first batch"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "0fdca13d6e364fedbe382ba105c06e3b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_47e39aa4a6854241b634bc1a170766c4",
              "IPY_MODEL_00c25d8bfe694d82b0478286ac73a746",
              "IPY_MODEL_aa4f666aab9a4700b4927fa3b6220af5"
            ],
            "layout": "IPY_MODEL_f2c944dea18a413fb6e07c341e137ffe"
          }
        },
        "47e39aa4a6854241b634bc1a170766c4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_aae4c681ca224d41aa9d4001b0da8a54",
            "placeholder": "​",
            "style": "IPY_MODEL_a541b53a0bbd44bd83e88a63c8ff92b9",
            "value": "[EPOCH 1/10] 3/178:   2%"
          }
        },
        "00c25d8bfe694d82b0478286ac73a746": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_04e531afc70045f8a7c5612c8e1b2580",
            "max": 178,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_670cc6f44204479db40653b83b14ab73",
            "value": 3
          }
        },
        "aa4f666aab9a4700b4927fa3b6220af5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fcb81569359a4b1baccc7e0c785ddb29",
            "placeholder": "​",
            "style": "IPY_MODEL_3dfec5bdf9ec4197971509afc2345974",
            "value": " 3/178 [00:28&lt;27:17,  9.36s/it, loss=1.43e+3, mAP=0]"
          }
        },
        "f2c944dea18a413fb6e07c341e137ffe": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "aae4c681ca224d41aa9d4001b0da8a54": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a541b53a0bbd44bd83e88a63c8ff92b9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "04e531afc70045f8a7c5612c8e1b2580": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "670cc6f44204479db40653b83b14ab73": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "fcb81569359a4b1baccc7e0c785ddb29": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3dfec5bdf9ec4197971509afc2345974": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}